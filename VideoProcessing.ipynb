#####################################################
#https://www.geeksforgeeks.org/how-to-use-google-colaboratory-for-video-processing/
#####################################################


from google.colab import drive 
drive.mount('/content/drive') 
    
import os, sys 
import random 
import math 
import numpy as np 
import skimage.io 
import matplotlib 
import matplotlib.pyplot as plt 
import cv2 
    
from matplotlib.patches import Polygon 
    
os.chdir("/content/drive/My Drive/Colab Notebooks/MRCNN_pure") 
sys.path.append("/content/drive/My Drive/Colab Notebooks/MRCNN_pure") 
VIDEO_STREAM = "/content/drive/My Drive/Colab Notebooks/Millery.avi"
VIDEO_STREAM_OUT = "/content/drive/My Drive/Colab Notebooks/Result.avi"
    
# Root directory of the project 
ROOT_DIR = os.path.abspath(".") 
    
# Import Mask RCNN 
sys.path.append(ROOT_DIR)  # To find local version of the library 
from mrcnn import utils 
import mrcnn.model as modellib 
from mrcnn import visualize 
# Import COCO config 
sys.path.append(os.path.join(ROOT_DIR, "samples/coco/"))  # To find local version 
import coco 
    
# Directory to save logs and trained model 
MODEL_DIR = os.path.join(ROOT_DIR, "logs") 
    
# Local path to trained weights file 
COCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5") 
# Download COCO trained weights from Releases if needed 
if not os.path.exists(COCO_MODEL_PATH): 
    utils.download_trained_weights(COCO_MODEL_PATH) 
    
# Directory of images to run detection on 
IMAGE_DIR = os.path.join(ROOT_DIR, "images") 
    
class InferenceConfig(coco.CocoConfig): 
    # Set batch size to 1 since we'll be running inference on 
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU 
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
        
        
def display_instances(image, boxes, masks, ids, names, scores): 
    """ 
        take the image and results and apply the mask, box, and Label 
    """
    n_instances = boxes.shape[0] 
    colors = visualize.random_colors(n_instances) 
    
    if not n_instances: 
        print('NO INSTANCES TO DISPLAY') 
    else: 
        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0] 
    
    for i, color in enumerate(colors): 
        if not np.any(boxes[i]): 
            continue
    
        y1, x1, y2, x2 = boxes[i] 
        label = names[ids[i]] 
        score = scores[i] if scores is not None else None
        caption = '{} {:.2f}'.format(label, score) if score else label 
        mask = masks[:, :, i] 
    
        image = visualize.apply_mask(image, mask, color) 
        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2) 
        image = cv2.putText( 
            image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2
        ) 
    
    return image 
    
    
config = InferenceConfig() 
config.display() 
    
# Create model object in inference mode. 
model = modellib.MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=config) 
    
# Load weights trained on MS-COCO 
model.load_weights(COCO_MODEL_PATH, by_name=True) 
    
# COCO Class names 
# Index of the class in the list is its ID. For example, to get ID of 
# the teddy bear class, use: class_names.index('teddy bear') 
class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 
               'bus', 'train', 'truck', 'boat', 'traffic light', 
               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 
               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 
               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 
               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 
               'kite', 'baseball bat', 'baseball glove', 'skateboard', 
               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 
               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 
               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 
               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 
               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 
               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 
               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 
               'teddy bear', 'hair drier', 'toothbrush'] 
    
# Initialize the video stream and pointer to output video file 
vs = cv2.VideoCapture(VIDEO_STREAM) 
writer = None
vs.set(cv2.CAP_PROP_POS_FRAMES, 1000); 
    
i = 0
while i < 20000: 
  # read the next frame from the file 
  (grabbed, frame) = vs.read() 
  i += 1
     
  # If the frame was not grabbed, then we have reached the end 
  # of the stream 
  if not grabbed: 
    print ("Not grabbed.") 
    break; 
      
  # Run detection 
  results = model.detect([frame], verbose=1) 
    
  # Visualize results 
  r = results[0] 
  masked_frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], 
                            class_names, r['scores']) 
      
  # Check if the video writer is None 
  if writer is None: 
    # Initialize our video writer 
    fourcc = cv2.VideoWriter_fourcc(*"XVID") 
    writer = cv2.VideoWriter(VIDEO_STREAM_OUT, fourcc, 30, 
      (masked_frame.shape[1], masked_frame.shape[0]), True) 
     
  # Write the output frame to disk 
  writer.write(masked_frame) 
      
# Release the file pointers 
print("[INFO] cleaning up...") 
writer.release() 











